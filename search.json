[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "SE-PINN",
    "section": "",
    "text": "Solving the Schrödinger Equation via Physics-Informed Machine Learning\n\n\nIntroduction\nSE-PINN is a physics-informed neural network in PyTorch that solves the Schrödinger equation of quantum mechanics.\nHere, I first explain how SE-PINN is designed and then demonstrate how SE-PINN is applied to the quantum harmonic oscillator, which is a model that is used for the interatomic bonding of molecules such as hydrogen iodide and hydrogen fluoride.\n\n\n\n\nHow SE-PINN is Designed\nAs a physics-informed neural network (PINN), SE-PINN uses principles from physics to reduce the space of solutions in which it searches.\nIn particular, four physical constraints are used: normality, orthogonality, symmetry, and consistency, all of which are explained in the subsequent sections.\nEach physical constraint is integrated into SE-PINN through one of two methods: (1) inclusion in the objective function of the model (which is used for normality, orthogonality, and consistency) or (2) exact conservation via the architecture of the model (which is used for symmetry).\nAn outstanding feature of SE-PINN is that it is not trained on a set of labeled examples — in other words via supervised learning. In contrast, it learns via reinforcement learning (RL) through direct feedback from the SE, which enables it to adapt to any quantum-mechanical system.\nIn fact, it is a minimal example of RL, defined by the Markov Decision Process (MDP) in which the state is the parameters of the model, the action is to compute its predictions based on the parameters, the instantaneous total reward is the negation of the loss, and the policy is to update the parameters to minimize the loss.\nIn addition, SE-PINN is trained with the L-BFGS algorithm, which is a second-order optimizer that approximates the Hessian matrix.\nLast, SE-PINN is organized into two classes: BasePINN, which implements the architecture of the model, and PINN, which is a wrapper of BasePINN that includes infrastructure for training and visualization.\nThe decoupling of the design enables users to define custom classes that change or extend SE-PINN.\n\n\nBackground\n\n(1) Physics\nQuantum mechanics is a theory of physics at the atomic scale.\nThe central equation of quantum mechanics is the Schrödinger equation, whose time-independent form is as follows.\n\\[ - \\frac{\\hbar^{2}}{2m} \\frac{d^2\\psi}{dx^2} + V \\psi = E \\psi \\]\n\n\\(\\hbar\\) is the reduced Planck constant.\n\\(m\\) is the mass of the system.\n\\(\\psi\\) is the wavefunction of the system.\n\\(x\\) is the spatial coordinate (position).\n\\(V\\) is the potential-energy field of the system.\n\\(E\\) is the energy of the system.\n\n\\(\\psi\\), the wavefunction of the system, is the representation of its state.\nIn classical mechanics, physical quantities such as position and momentum can be measured with infinite precision (in theory). In quantum mechanics, however, the wavefunction is an intermediary between physical quantities and measurements of them.\nHeisenberg’s uncertainty principle, for example, indicates that simultaneous measurements of position and momentum are limited in precision.\n\n\n\n(2) Mathematics\nMatrices of complex numbers are central in quantum mechanics.\nThe complex conjugate, \\(z^{*}\\), of a complex number, \\(z\\), is the complex number whose real part is identical and whose imaginary part is equal in magnitude and opposite in sign. In other words, the complex conjugate of \\(a + bi\\) is \\(a - bi\\), and vice versa.\nThe modulus, \\(|z|\\), of a complex number, \\(z = a + bi\\), is equal to \\(\\sqrt{a^2 + b^2}\\). Moreover, the square of the modulus is equal to the product of \\(z\\) and its complex conjugate: \\(|z|^2 = z^{*}z\\).\nThe conjugate transpose, \\(A^{*}\\), of a complex matrix, \\(A\\), is its transpose where each element is instead its complex conjugate.\nA Hermitian matrix is a matrix that is equal to its conjugate transpose. In other words, \\(A = A^{*}\\).\nExtension of modulus to complex vectors / matrices.\n\n\n\n\nProperty 1: Normality\nPhysics: In the Copenhagen interpretation of quantum mechanics, the wavefunction of a physical system determines the probability that the system is observed to be in a particular state. In particular, the squared modulus of the wavefunction, \\(|\\psi(x)|^{2}\\), is a probability density function.\n\\[\\int_{a}^{b} |\\psi(x)|^{2} \\,dx = \\Pr[a \\leq X \\leq b]\\]\nMathematics: The integral of such a quantity over all states must be equal to 1 due to the law of total probability. Furthermore, for systems that are bound to a potential and therefore localized in space, the limit of the wavefunction at infinity must be equal to 0.\n\n\\[\\int_{-\\infty}^{+\\infty} |\\psi(x)|^{2} \\,dx = 1\\]\n\\[ \\lim_{x\\to\\pm\\infty} \\psi(x) = 0 \\]\nSE-PINN: The objective function of the model includes both a normality-based component that is adjusted for discretization (dx) and a boundary-based component.\nnormality_loss = (torch.sum(wf ** 2) - 1 / self.dx) ** 2\n\n\nDerivation\n\n\n\n\\[\\begin{equation}\n\\begin{split}\n\\int_{-\\infty}^{+\\infty} |\\psi(x)|^{2} \\,dx & = 1 \\\\[1em]\n\\sum_{i} |\\psi(x_{i})|^{2} \\,\\Delta x & = 1 \\\\[1em]\n\\sum_{i} |\\psi(x_{i})|^{2} & = \\frac{1}{\\Delta x} \\\\[1em]\n\\sum_{i} |\\psi(x_{i})|^{2} - \\frac{1}{\\Delta x} & = 0 \\\\[1em]\n\\left( \\sum_{i} |\\psi(x_{i})|^{2} - \\frac{1}{\\Delta x} \\right)^{2} & = 0\n\\end{split}\n\\end{equation}\\]\n\n\n\n\n\nExplanation\n\n\n\nThe final squaring is used so that positive values and negative values are additive rather than subtractive and so that high values are penalized more. But another function such as the absolute value can be used as well.\nMoreover, such a loss discourages trivial solutions (wavefunctions of 0) since they result in non-zero loss, \\(\\left(\\frac{1}{\\Delta x}\\right)^{2}\\), while non-trivial solutions result in zero loss.\n\n\n\nboundary_loss = wf[0] ** 2 + wf[-1] ** 2\n\n\nExplanation\n\n\n\nwf[0] is the value at the left end of the wavefunction, and wf[-1] is the value at the right end of the wavefunction.\nThe squaring of each end is used so that (1) positive values and negative values are additive rather than subtractive and (2) high values are penalized more.\n\n\n\n\n\n\nProperty 2: Orthogonality\nPhysics: Measurements of a physical quantity, such as energy, are real numbers rather than complex numbers.\n\\[E \\in \\mathbb{R}\\]\nMathematics: Measurements of a physical quantity are eigenvalues of a complex matrix that represents the physical quantity. Since these eigenvalues are real, the matrix must be Hermitian. Thus, as a property of Hermitian matrices, the set of all eigenvectors of such a matrix must be an orthogonal set; in other words, every eigenvector must be orthogonal to every other eigenvector.\n\\[ \\psi_{i}(x)^{*} \\psi_{j}(x) = 0, \\ i \\neq j\\]\n\n\nProof\n\n\n\nThe proof follows from the fact that the Hamiltonian matrix, \\(H\\), is Hermitian (\\(H = H^{*}\\)), that the indices of the eigenvectors are distinct (\\(i \\neq j\\)), and that the eigenvalues of such eigenvectors are non-degenerate (\\(E_{i} \\neq E_{j}\\)).\n\\[\\begin{equation}\n\\begin{split}\n\n\\psi_{i}(x)^{*} \\hat{H}^{*} \\psi_{j}(x) & = \\psi_{i}(x)^{*} \\hat{H}^{*} \\psi_{j}(x) \\\\[1em]\n\n\\left(\\hat{H} \\psi_{i}(x)\\right)^{*} \\psi_{j}(x) & = \\psi_{i}(x)^{*} \\left(\\hat{H} \\psi_{j}(x)\\right) \\\\[1em]\n\nE_{i} \\left(\\psi_{i}(x)^{*} \\psi_{j}(x)\\right) & = E_{j} \\left(\\psi_{i}(x)^{*} \\psi_{j}(x)\\right) \\\\[1em]\n\nE_{i} \\left(\\psi_{i}(x)^{*} \\psi_{j}(x)\\right) - E_{j} \\left(\\psi_{i}(x)^{*} \\psi_{j}(x)\\right) & = 0 \\\\[1em]\n\n(E_{i} - E_{j}) \\left(\\psi_{i}(x)^{*} \\psi_{j}(x)\\right) & = 0 \\\\[1em]\n\n\\psi_{i}(x)^{*} \\psi_{j}(x) & = 0\n\n\\end{split}\n\\end{equation}\\]\n\n\n\nSE-PINN: The objective function of the model includes an orthogonality-based component.\northogonality_loss = torch.dot(wf, self.basis_sum) ** 2\n\n\nExplanation\n\n\n\nself.basis_sum is the sum of all other eigenvectors so far learned by SE-PINN, and it is therefore a linear combination of these eigenvectors. As a result, any vector that is orthogonal to self.basis_sum is orthogonal to each of the other eigenvectors. The great advantage is that computing the inner product of the current wavefunction and self.basis_sum is much more efficient than computing it for each pair.\nThe final squaring is used so that positive values and negative values are additive rather than subtractive and so that high values are penalized more. But another function such as the absolute value can be used as well.\n\n\n\n\n\nProperty 3: Symmetry\nPhysics: If the quantum-mechanical potential of a system is even, in other words symmetric about the y-axis such that \\(V(x) = V(-x)\\), then the eigenvectors of energy that are bound to the potential are also symmetric, either even or odd.\n\n\nProof\n\n\n\nThe proof follows from the Schrödinger equation.\n\\[\\begin{equation}\n\\begin{split}\n\n- \\frac{\\hbar^{2}}{2m} \\frac{d^2\\psi(x)}{dx^2} + V(x) \\psi(x) & = E \\psi(x) \\\\[1em]\n- \\frac{\\hbar^{2}}{2m} \\frac{d^2\\psi(-x)}{d(-x)^2} + V(-x) \\psi(-x) & = E \\psi(-x) \\\\[1em]\n- \\frac{\\hbar^{2}}{2m} \\frac{d^2\\psi(-x)}{dx^2} + V(x) \\psi(-x) & = E \\psi(-x)\n\n\\end{split}\n\\end{equation}\\]\nThus, both \\(\\psi(x)\\) and \\(\\psi(-x)\\) are solutions of the same energy, \\(E\\).\nHowever, since eigenvectors that are bound to the potential are non-degenerate, \\(\\psi(x)\\) and \\(\\psi(-x)\\) correspond to the same solution.\nMoreover, since the Schrödinger equation is a linear equation, \\(\\psi(x) = a \\psi(-x)\\), and both \\(a = 1\\) and \\(a = -1\\) are valid.\nAs a result, \\(\\psi(x) = \\psi(-x)\\) is an even solution, \\(\\psi(x) = -\\psi(-x)\\) is an odd solution, and every solution is either even or odd.\n\n\n\nMathematics: Any function whose domain is symmetric about the origin can be expressed as the sum of an even function and an odd function, and therefore these components can be separated.\n\n\n\\[ \\psi_{\\text{E}}(x) = \\frac{1}{2} (\\psi(x) + \\psi(-x)) \\]\n\\[ \\psi_{\\text{O}}(x) = \\frac{1}{2} (\\psi(x) - \\psi(-x)) \\]\n\\[ \\psi(x) = \\psi_{\\text{E}}(x) + \\psi_{\\text{O}}(x) \\]\n\n\nProof\n\n\n\nAs indicated above, the domain of \\(\\psi(x)\\) must be symmetric about the origin for such a decomposition to exist. Otherwise \\(\\psi(-x)\\) is not defined for all values of \\(x\\) in the domain!\n\n\\(\\psi_{\\text{E}}(x) + \\psi_{\\text{O}}(x)\\) is a decomposition of \\(\\psi(x)\\).\n\n\\[\\begin{equation}\n\\begin{split}\n\\psi_{\\text{E}}(x) + \\psi_{\\text{O}}(x) & = \\frac{1}{2} (\\psi(x) + \\psi(-x)) + \\frac{1}{2} (\\psi(x) - \\psi(-x)) \\\\[1em]\n& = \\frac{1}{2} \\psi(x) + \\frac{1}{2} \\psi(-x) + \\frac{1}{2} \\psi(x) - \\frac{1}{2} \\psi(-x) \\\\[1em]\n& = \\psi(x)\n\\end{split}\n\\end{equation}\\]\n\n\\(\\psi_{\\text{E}}(x)\\) is even.\n\n\\[\\begin{equation}\n\\begin{split}\n\\psi_{\\text{E}}(x) & = \\frac{1}{2} (\\psi(x) + \\psi(-x)) \\\\[1em]\n& = \\frac{1}{2} (\\psi(-x) + \\psi(x)) \\\\[1em]\n& = \\psi_{\\text{E}}(-x)\n\\end{split}\n\\end{equation}\\]\n\n\\(\\psi_{\\text{O}}(x)\\) is odd.\n\n\\[\\begin{equation}\n\\begin{split}\n\\psi_{\\text{O}}(x) & = \\frac{1}{2} (\\psi(x) - \\psi(-x)) \\\\[1em]\n& = -\\frac{1}{2} (\\psi(-x) - \\psi(x)) \\\\[1em]\n& = -\\psi_{\\text{O}}(-x)\n\\end{split}\n\\end{equation}\\]\n\n\n\nSE-PINN: A custom architectural layer that can enforce such a separation — a hub layer — is used as the final layer of the model.\npredicted_wf = self.even * 0.5 * (torch.mm(self.weights, H_plus) + 2 * self.bias)\n             + self.odd * 0.5 * torch.mm(self.weights, H_minus)\n\n\nDerivation\n\n\n\n\\[\\psi(x) = \\sum_{i = 1}^{N} w_{i} h_{i}(x) + b\\]\n\n\\[\\begin{equation}\n\\begin{split}\n\\psi_{\\text{E}}(x) & = \\frac{1}{2} (\\psi(x) + \\psi(-x)) \\\\[1em]\n\n& = \\frac{1}{2} \\left(\\sum_{i = 1}^{N} w_{i} h_{i}(x) + b + \\sum_{i = 1}^{N} w_{i} h_{i}(-x) + b\\right) \\\\[1em]\n\n& = \\frac{1}{2} \\left(\\sum_{i = 1}^{N} (w_{i} h_{i}(x) + w_{i} h_{i}(-x)) + 2 b\\right) \\\\[1em]\n\n& = \\frac{1}{2} \\left(\\sum_{i = 1}^{N} w_{i} (h_{i}(x) + h_{i}(-x)) + 2 b\\right) \\\\[1em]\n\n& = \\frac{1}{2} \\left(\\sum_{i = 1}^{N} w_{i} H_{i}^{+}(x) + 2 b\\right)\n\\end{split}\n\\end{equation}\\]\n\n\\[\\begin{equation}\n\\begin{split}\n\\psi_{\\text{O}}(x) & = \\frac{1}{2} (\\psi(x) - \\psi(-x)) \\\\[1em]\n\n& = \\frac{1}{2} \\left(\\sum_{i = 1}^{N} w_{i} h_{i}(x) + b - \\left(\\sum_{i = 1}^{N} w_{i} h_{i}(-x) + b\\right)\\right) \\\\[1em]\n\n& = \\frac{1}{2} \\left(\\sum_{i = 1}^{N} (w_{i} h_{i}(x) - w_{i} h_{i}(-x))\\right) \\\\[1em]\n\n& = \\frac{1}{2} \\left(\\sum_{i = 1}^{N} w_{i} (h_{i}(x) - h_{i}(-x))\\right) \\\\[1em]\n\n& = \\frac{1}{2} \\left(\\sum_{i = 1}^{N} w_{i} H_{i}^{-}(x)\\right)\n\\end{split}\n\\end{equation}\\]\n\n\\[\\begin{equation}\n\\begin{split}\n\\psi(x) & = \\psi_{\\text{E}}(x) + \\psi_{\\text{O}}(x) \\\\[1em]\n\n& = \\frac{1}{2} \\left(\\sum_{i = 1}^{N} w_{i} H_{i}^{+}(x) + 2 b\\right) + \\frac{1}{2} \\left(\\sum_{i = 1}^{N} w_{i} H_{i}^{-}(x)\\right)\n\\end{split}\n\\end{equation}\\]\n\n\n\n\n\nExplanation\n\n\n\nself.even and self.odd can be configured as 0 or 1 to enforce a particular symmetry.\nSince SE-PINN is a neural network, \\(\\psi(x) = \\sum_{i = 1}^{N} w_{i} h_{i}(x) + b\\), where \\(w_{i}\\) is a weight of the final hidden layer, \\(h_{i}(x)\\) is an input from the previous layer, and \\(b\\) is the bias of the output layer (a single node).\nH_plus and H_minus are used to abbreviate the notation.\nH_plus is equal to \\(h(x) + h(-x)\\), and H_minus is equal to \\(h(x) - h(-x)\\).\n\n\n\n\n\nProperty 4: Consistency\nPhysics: Any state of a quantum-mechanical system must satisfy the Schrödinger equation.\nMathematics: The time-independent Schrödinger equation is a differential equation, whose solutions, \\(\\psi\\), are eigenvectors of the Hamiltonian matrix, \\(\\hat{H} = - \\frac{\\hbar^{2}}{2m} \\frac{d^2}{dx^2} + V\\).\n\\[\\begin{equation}\n\\begin{split}\n\\hat{H} \\psi & = E \\psi \\\\[1em]\n- \\frac{\\hbar^{2}}{2m} \\frac{d^2\\psi}{dx^2} + V \\psi & = E \\psi\n\\end{split}\n\\end{equation}\\]\nSE-PINN: The objective function of the model includes a component that quantifies how well it satisfies the Schrödinger equation via the mean squared error (MSE).\nSE_loss = torch.mean((-hbar / (2 * self.m) * dd + self.V * wf - energy * wf) ** 2)\n\n\nDerivation\n\n\n\n\\(\\hbar^{2}\\) and \\(m\\) can be omitted for simplicity and absorbed by \\(E\\).\n\\[\\begin{equation}\n\\begin{split}\n- \\frac{1}{2} \\frac{d^2\\psi}{dx^2} + V \\psi & = E \\psi \\\\[1em]\n- \\frac{1}{2} \\frac{d^2\\psi}{dx^2} + V \\psi - E \\psi & = 0 \\\\[1em]\n\\end{split}\n\\end{equation}\\]\n\n\n\n\n\nExplanation\n\n\n\ndd is the second derivative of the wavefunction with respect to position and is of the same shape as the wavefunction. The potential, self.V, is a scalar-valued function, and energy is a scalar, so self.V * wf and energy * wf are element-wise products.\n\n\n\n\n\n\nHow SE-PINN is Applied\n\nStep 1: Initialize the Environment\n%config InlineBackend.print_figure_kwargs = {'bbox_inches': None}\n\nimport os\nimport random\nimport sys\n\nimport IPython\nfrom matplotlib.animation import FuncAnimation, PillowWriter\nimport matplotlib_inline\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom rich.progress import (\n    BarColumn,\n    Progress,\n    TaskProgressColumn,\n    TextColumn,\n    TimeRemainingColumn,\n    track\n)\nfrom scipy.linalg import eigh_tridiagonal\nimport torch\nimport torch.nn as nn\n\nmatplotlib_inline.backend_inline.set_matplotlib_formats('retina')\nplt.rcParams['figure.figsize'] = (6.4, 4.8)\n\n# Environment Variables\nif 'google.colab' not in sys.modules:\n    !export LC_ALL='en_US.UTF-8'\n    !export LD_LIBRARY_PATH='/usr/lib64-nvidia'\n    !export LIBRARY_PATH='/usr/local/cuda/lib64/stubs'\n\n# Optional Hardware Acceleration\n# Use Runtime &gt; Change runtime type &gt; T4 GPU on Google Colab.\nif torch.cuda.is_available():\n    torch.cuda.init()\n    torch.cuda.is_initialized()\n    torch.set_default_tensor_type('torch.cuda.FloatTensor')\n    device = 'cuda'\nelse:\n    device = 'cpu'\ndevice = torch.device(device)\nprint(f'Using {device}.')\n\n# Settings for Reproducibility\ntorch.manual_seed(0)\ntorch.backends.cudnn.benchmark = False\ntorch.use_deterministic_algorithms(True)\ntorch.utils.deterministic.fill_uninitialized_memory = True\nos.environ['CUBLAS_WORKSPACE_CONFIG']=':4096:2' # cuBLAS\nnp.random.seed(0)\nrandom.seed(0)\n\n# Convenience Function for Plotting with PyTorch Tensors\ndef to_plot(x): return x.detach().cpu().numpy()\n\n\n\nStep 2: Define the BasePINN Class\nclass BasePINN(nn.Module):\n    \"\"\"\n    A base class for a physics-informed neural network (PINN) for\n    solving the Schrodinger equation.\n\n    Attributes\n    ----------\n    x0 : float\n        The spatial position of the leftmost point of the\n        quantum-mechanical potential.\n    xN : float\n        The spatial position of the rightmost point of the\n        quantum-mechanical potential.\n    dx : float\n        The uniform spatial Euclidean distance between adjacent points.\n    N : int\n        The count of points of the quantum-mechanical potential.\n    activation : builtin_function_or_method\n        The activation function.\n    sym : int\n        Whether to enforce even symmetry (1) or odd symmetry (-1) or not\n        to enforce symmetry (0).\n\n    Methods\n    -------\n    swap_symmetry\n        Swap the symmetry of the prediction of the model between even\n        symmetry and odd symmetry.\n\n    forward(x)\n        Forward pass.\n    \"\"\"\n\n    def __init__(self, grid_params, activation, sym=0):\n        super(BasePINN, self).__init__()\n\n        self.x0, self.xN, self.dx, self.N = grid_params\n        self.activation = activation\n        self.sym = sym\n\n        # Architecture of the Model\n\n        self.energy_node = nn.Linear(1, 1)\n\n        self.fc1_bypass = nn.Linear(1, 50)\n        self.fc1 = nn.Linear(2, 50)\n        self.fc2 = nn.Linear(50, 50)\n\n        # Selection of the Output Layer\n        if sym == 1:\n            # Enforcement of Even Symmetry\n            self.output_layer = HubLayer(50, 1, 1, 0)\n        elif sym == -1:\n            # Enforcement of Odd Symmetry\n            self.output_layer = HubLayer(50, 1, 0, 1)\n        else:\n            # No Enforcement of Symmetry\n            self.output_layer = nn.Linear(50, 1)\n\n    def swap_symmetry(self):\n        if self.sym == 0:\n            print('Symmetry cannot be swapped because it is not enforced.')\n            return\n        self.output_layer.flip_sym()\n\n    def forward(self, x):\n        # Lambda Layer for Energy\n        energy = self.energy_node(torch.ones_like(x))\n\n        N = torch.cat((x, energy), dim=1)\n        N = self.activation(self.fc1(N))\n        N = self.activation(self.fc2(N))\n        wf = self.output_layer(N) # Possible enforcement of symmetry.\n\n        return wf, energy\n\nclass HubLayer(nn.Module):\n    \"\"\"\n    A hub layer, which is used to constrain the prediction of the model\n    to respect even symmetry (symmetry about f(x) = 0) or odd symmetry\n    (symmetry about f(x) = x). The mathematical basis is presented at\n    https://arxiv.org/pdf/1904.08991.pdf. The constructor is adapted\n    from https://auro-227.medium.com/writing-a-custom-layer-in-pytorch-14ab6ac94b77.\n\n    Attributes\n    ----------\n    size_in : int\n        The length of the input of the layer.\n    size_out : int\n        The length of the output of the layer.\n    weights : torch.nn.parameter.Parameter\n        The weights of the layer.\n    bias : torch.nn.parameter.Parameter\n        The bias of the layer.\n    even : int\n        1 to enforce even symmetry.\n    odd : int\n        -1 to enforce odd symmetry.\n\n    Methods\n    -------\n    flip_sym\n        Swap the symmetry between even symmetry and odd symmetry.\n\n    forward(x)\n        Forward pass.\n    \"\"\"\n\n    def __init__(self, size_in, size_out, even, odd):\n        super().__init__()\n\n        self.size_in, self.size_out = size_in, size_out\n\n        weights = torch.Tensor(size_out, size_in)\n        self.weights = nn.Parameter(weights)\n\n        bias = torch.Tensor(size_out)\n        self.bias = nn.Parameter(bias)\n\n        self.even = even\n        self.odd = odd\n\n        # Initialization of Weights (Kaiming Initialization)\n        nn.init.kaiming_uniform_(self.weights, a=np.sqrt(5))\n\n        # Initialization of Biases (LeCun Initialization)\n        fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weights)\n        bound = 1 / np.sqrt(fan_in)\n        nn.init.uniform_(self.bias, -bound, bound)\n\n    def flip_sym(self):\n        self.even = 1 - self.even\n        self.odd = 1 - self.odd\n        return\n\n    def forward(self, x):\n        h_plus = x # This is x(t).\n        h_minus = torch.flip(x, [0]) # This is x(-t).\n\n        H_plus = h_plus + h_minus\n        H_minus = h_plus - h_minus\n\n        N = ((self.even * (1/2) * torch.mm(H_plus, self.weights.t()))\n           + (self.odd * (1/2) * torch.mm(H_minus, self.weights.t())))\n\n        return N\n\n\n\nStep 3: Define the PINN Class\n\nWithout PlottingWith Plotting\n\n\nclass PINN():\n    \"\"\"\n    An implementation of a physics-informed neural network (PINN) for\n    solving the Schrodinger equation with infrastructure for training\n    and visualization.\n\n    Attributes\n    ----------\n    x : torch.Tensor\n        The numerical grid of the physical system.\n    x0 : float\n        The leftmost point of the numerical grid.\n    xN : float\n        The rightmost point of the numerical grid.\n    N : int\n        The count of points that the numerical grid has.\n    V : torch.Tensor\n        The potential.\n    basis : list\n        The basis.\n    basis_sum : torch.Tensor\n        The sum of the eigenvectors of the basis.\n    cur_loss : float\n        The current loss of the model.\n    cur_energy : float\n        The current prediction of the energy eigenvalue from the model.\n    cur_wf : torch.Tensor\n        The current prediction of the energy eigenvector from the model.\n    losses : list\n        A list of all losses of the model.\n    energies : list\n        A list of all energy eigenvalues of the model.\n    wfs : list\n        A list of all energy eigenvectors of the model.\n\n    Methods\n    -------\n    init_optimizer(optimizer_name='LBFGS', lr=1e-3)\n        Initialize the optimizer for training the model.\n    change_lr(lr)\n        Change the learning rate for training the model.\n    swap_symmetry\n        Swap the symmetry of the model.\n    add_to_basis(base=None)\n        Add the predicted energy eigenvector to the basis.\n    closure\n        Necessary for computing the loss with the L-BFGS optimizer.\n    loss_fn(x)\n        Computes the loss of the model.\n    train(epochs=10)\n        Trains the model.\n    plot(metrics=['loss', 'energy', 'wf'], ref_energy=None, ref_wf=None)\n        Plot a set of metrics.\n    plot_loss\n        Plot the loss.\n    plot_energy(ref_energy=None)\n        Plot the energy eigenvalue that is predicted by the model.\n    plot_wf(idx=None, ref_wf=None)\n        Plot the energy eigenvector that is predicted by the model.\n    animate(filename, ref_energy=None, ref_wf=None, epoch_range=None,\n            display=False)\n        Plot the predictions of the model as an animation.\n    \"\"\"\n\n    def __init__(self, grid_params, activation, potential, sym):\n        self.x0, self.xN, self.dx, self.N = grid_params\n        self.x = torch.linspace(self.x0, self.xN, self.N - 1).view(-1, 1)\n        self.V = potential\n\n        self.model = BasePINN(grid_params, activation, sym)\n        self.model.to(device)\n\n        # Persistent information about the predicted basis.\n        self.basis = []\n        self.basis_sum = torch.zeros_like(self.x)\n\n        # Current values of metrics.\n        self.cur_loss = 0\n        self.cur_energy = 0\n        self.cur_wf = 0\n\n        # All values of metrics.\n        self.losses = []\n        self.energies = []\n        self.wfs = []\n\n    def init_optimizer(self, optimizer_name='LBFGS', lr=1e-3):\n        if optimizer_name == 'LBFGS':\n            self.optimizer = torch.optim.LBFGS(self.model.parameters(), lr=lr)\n        elif optimizer_name == 'Adam':\n            self.optimizer = torch.optim.Adam(self.model.parameters(), lr=lr)\n        else:\n            print('The name of the optimizer is invalid.')\n            return\n\n        self.optimizer_name = optimizer_name\n\n    def change_lr(self, lr):\n        \"\"\"\n        On-the-fly (runtime) control of the learning rate.\n        \"\"\"\n\n        self.optimizer.param_groups[0]['lr'] = lr\n\n    def swap_symmetry(self):\n        \"\"\"\n        On-the-fly (runtime) control of the enforced symmetry.\n        \"\"\"\n\n        self.model.swap_symmetry()\n\n    def add_to_basis(self, base=None):\n        if base is None:\n            base = self.cur_wf.clone().detach()\n\n        self.basis.append(base)\n        self.basis_sum += base\n\n    def closure(self):\n        \"\"\"\n        The closure method is necessary for the L-BFGS optimizer since\n        it evaluates the loss of the model at multiple points in\n        parameter space at each step of training in contrast with the\n        other optimizers in PyTorch.\n        \"\"\"\n\n        self.optimizer.zero_grad()\n        loss = self.loss_fn(self.x)\n        loss.backward()\n        return loss\n\n    def loss_fn(self, x):\n        self.x.requires_grad = True\n\n        wf, energy = self.model(self.x)\n\n        # First Derivative\n        d = torch.autograd.grad(wf.sum(), x, create_graph=True)[0]\n        # Second Derivative\n        dd = torch.autograd.grad(d.sum(), x, create_graph=True)[0]\n\n        # SE Loss\n        SE_loss = torch.sum((-0.5 * dd + self.V * wf - energy * wf) ** 2)\n        SE_loss /= self.N\n\n        # Normality Loss\n        normality_loss = (torch.sum(wf ** 2) - 1 / self.dx) ** 2\n\n        # Orthogonality Loss\n        orthogonality_loss = (torch.sum(wf * self.basis_sum) * self.dx) ** 2\n\n        # Boundary Loss\n        boundary_loss = 0.5 * (wf[0] ** 2 + wf[-1] ** 2)\n\n        # Total Loss\n        loss = SE_loss + normality_loss + orthogonality_loss + boundary_loss\n\n        self.cur_wf = wf\n        self.cur_energy = energy[0].item()\n        self.cur_loss = loss.item()\n\n        return loss\n\n    def train(self, epochs=10):\n        for _ in track(range(epochs), description='Training... '):\n            if self.optimizer_name == 'LBFGS':\n                loss = self.optimizer.step(self.closure)\n\n                if loss.item() == torch.nan:\n                    print('The loss is NAN.')\n                    break\n            elif self.optimizer_name == 'Adam':\n                self.optimizer.zero_grad()\n                loss = self.loss_fn(self.x)\n                loss.backward()\n                self.optimizer.step()\n\n            self.wfs.append(self.cur_wf)\n            self.energies.append(self.cur_energy)\n            self.losses.append(self.cur_loss)\n\n\nclass PINN():\n    \"\"\"\n    An implementation of a physics-informed neural network (PINN) for\n    solving the Schrodinger equation with infrastructure for training\n    and visualization.\n\n    Attributes\n    ----------\n    x : torch.Tensor\n        The numerical grid of the physical system.\n    x0 : float\n        The leftmost point of the numerical grid.\n    xN : float\n        The rightmost point of the numerical grid.\n    N : int\n        The count of points that the numerical grid has.\n    V : torch.Tensor\n        The potential.\n    basis : list\n        The basis.\n    basis_sum : torch.Tensor\n        The sum of the eigenvectors of the basis.\n    cur_loss : float\n        The current loss of the model.\n    cur_energy : float\n        The current prediction of the energy eigenvalue from the model.\n    cur_wf : torch.Tensor\n        The current prediction of the energy eigenvector from the model.\n    losses : list\n        A list of all losses of the model.\n    energies : list\n        A list of all energy eigenvalues of the model.\n    wfs : list\n        A list of all energy eigenvectors of the model.\n\n    Methods\n    -------\n    init_optimizer(optimizer_name='LBFGS', lr=1e-3)\n        Initialize the optimizer for training the model.\n    change_lr(lr)\n        Change the learning rate for training the model.\n    swap_symmetry\n        Swap the symmetry of the model.\n    add_to_basis(base=None)\n        Add the predicted energy eigenvector to the basis.\n    closure\n        Necessary for computing the loss with the L-BFGS optimizer.\n    loss_fn(x)\n        Computes the loss of the model.\n    train(epochs=10)\n        Trains the model.\n    plot(metrics=['loss', 'energy', 'wf'], ref_energy=None, ref_wf=None)\n        Plot a set of metrics.\n    plot_loss\n        Plot the loss.\n    plot_energy(ref_energy=None)\n        Plot the energy eigenvalue that is predicted by the model.\n    plot_wf(idx=None, ref_wf=None)\n        Plot the energy eigenvector that is predicted by the model.\n    animate(filename, ref_energy=None, ref_wf=None, epoch_range=None,\n            display=False)\n        Plot the predictions of the model as an animation.\n    \"\"\"\n\n    def __init__(self, grid_params, activation, potential, sym):\n        self.x0, self.xN, self.dx, self.N = grid_params\n        self.x = torch.linspace(self.x0, self.xN, self.N - 1).view(-1, 1)\n        self.V = potential\n\n        self.model = BasePINN(grid_params, activation, sym)\n        self.model.to(device)\n\n        # Persistent information about the predicted basis.\n        self.basis = []\n        self.basis_sum = torch.zeros_like(self.x)\n\n        # Current values of metrics.\n        self.cur_loss = 0\n        self.cur_energy = 0\n        self.cur_wf = 0\n\n        # All values of metrics.\n        self.losses = []\n        self.energies = []\n        self.wfs = []\n\n    def init_optimizer(self, optimizer_name='LBFGS', lr=1e-3):\n        if optimizer_name == 'LBFGS':\n            self.optimizer = torch.optim.LBFGS(self.model.parameters(), lr=lr)\n        elif optimizer_name == 'Adam':\n            self.optimizer = torch.optim.Adam(self.model.parameters(), lr=lr)\n        else:\n            print('The name of the optimizer is invalid.')\n            return\n\n        self.optimizer_name = optimizer_name\n\n    def change_lr(self, lr):\n        \"\"\"\n        On-the-fly (runtime) control of the learning rate.\n        \"\"\"\n\n        self.optimizer.param_groups[0]['lr'] = lr\n\n    def swap_symmetry(self):\n        \"\"\"\n        On-the-fly (runtime) control of the enforced symmetry.\n        \"\"\"\n\n        self.model.swap_symmetry()\n\n    def add_to_basis(self, base=None):\n        if base is None:\n            base = self.cur_wf.clone().detach()\n\n        self.basis.append(base)\n        self.basis_sum += base\n\n    def closure(self):\n        \"\"\"\n        The closure method is necessary for the L-BFGS optimizer since\n        it evaluates the loss of the model at multiple points in\n        parameter space at each step of training in contrast with the\n        other optimizers in PyTorch.\n        \"\"\"\n\n        self.optimizer.zero_grad()\n        loss = self.loss_fn(self.x)\n        loss.backward()\n        return loss\n\n    def loss_fn(self, x):\n        self.x.requires_grad = True\n\n        wf, energy = self.model(self.x)\n\n        # First Derivative\n        d = torch.autograd.grad(wf.sum(), x, create_graph=True)[0]\n        # Second Derivative\n        dd = torch.autograd.grad(d.sum(), x, create_graph=True)[0]\n\n        # SE Loss\n        SE_loss = torch.sum((-0.5 * dd + self.V * wf - energy * wf) ** 2)\n        SE_loss /= self.N\n\n        # Normality Loss\n        normality_loss = (torch.sum(wf ** 2) - 1 / self.dx) ** 2\n\n        # Orthogonality Loss\n        orthogonality_loss = (torch.sum(wf * self.basis_sum) * self.dx) ** 2\n\n        # Boundary Loss\n        boundary_loss = 0.5 * (wf[0] ** 2 + wf[-1] ** 2)\n\n        # Total Loss\n        loss = SE_loss + normality_loss + orthogonality_loss + boundary_loss\n\n        self.cur_wf = wf\n        self.cur_energy = energy[0].item()\n        self.cur_loss = loss.item()\n\n        return loss\n\n    def train(self, epochs=10):\n        for _ in track(range(epochs), description='Training... '):\n            if self.optimizer_name == 'LBFGS':\n                loss = self.optimizer.step(self.closure)\n\n                if loss.item() == torch.nan:\n                    print('The loss is NAN.')\n                    break\n            elif self.optimizer_name == 'Adam':\n                self.optimizer.zero_grad()\n                loss = self.loss_fn(self.x)\n                loss.backward()\n                self.optimizer.step()\n\n            self.wfs.append(self.cur_wf)\n            self.energies.append(self.cur_energy)\n            self.losses.append(self.cur_loss)\n\n    def plot(self, metrics=['loss', 'energy', 'wf'], ref_energy=None,\n             ref_wf=None):\n        def route(metric):\n            if metric == 'loss':\n                self.plot_loss()\n            elif metric == 'energy':\n                self.plot_energy(ref_energy=ref_energy)\n            elif metric == 'wf':\n                self.plot_wf(ref_wf=ref_wf)\n            else:\n                message = 'The metric must be \\'loss\\', \\'energy\\', or \\'wf\\' '\n                message += f'rather than {repr(metric)}.'\n                print(message)\n\n        if isinstance(metrics, str):\n            route(metrics)\n        elif isinstance(metrics, list):\n            for metric in metrics:\n                route(metric)\n        else:\n            message = f'The type of the metrics parameter must be {repr(str)} '\n            message += f'or {repr(list)} rather than {type(metrics)}.'\n            print(message)\n\n    def plot_loss(self):\n        _ = plt.figure(figsize=(6.4, 4.8))\n        plt.plot(self.losses)\n        plt.yscale('log')\n        plt.title('Loss during Training', loc='left')\n        plt.xlabel('Epoch')\n        plt.ylabel('Loss')\n        plt.subplots_adjust(left=0.2, right=0.95)\n        if len(self.losses) &lt; 10:\n            plt.xticks(range(len(self.losses)))\n        plt.grid(alpha=0.2, which='both')\n        plt.show()\n        plt.close()\n\n    def plot_energy(self, ref_energy=None):\n        _ = plt.figure(figsize=(6.4, 4.8))\n        plt.plot(self.energies)\n        if ref_energy is not None:\n            plt.axhline(ref_energy, color='k', linestyle='--',\n                        label='Ground Truth')\n        plt.title('Energy Eigenvalue during Training', loc='left')\n        plt.xlabel('Epoch')\n        plt.ylabel('Energy Eigenvalue')\n        plt.subplots_adjust(left=0.15, right=0.95)\n        if len(self.energies) &lt; 10:\n            plt.xticks(range(len(self.energies)))\n        plt.grid(alpha=0.2)\n        plt.show()\n        plt.close()\n\n    def plot_wf(self, idx=None, ref_wf=None):\n        _ = plt.figure(figsize=(6.4, 4.8))\n\n        if idx is None:\n            psi = self.cur_wf\n            energy = self.cur_energy\n        else:\n            psi = self.wfs[idx]\n            energy = self.energies[idx]\n        norm = torch.sum(psi ** 2) * self.dx\n\n        plt.plot(to_plot(self.x), to_plot(psi), 'r-', label='Prediction')\n        plt.plot(to_plot(self.x), -to_plot(psi), 'b-', label='- Prediction')\n\n        if ref_wf is not None:\n            plt.plot(to_plot(self.x), ref_wf, 'k--', label='Ground Truth')\n\n        title = f'Energy Eigenvector (Norm of {norm:.2f} and Energy of '\n        title += f'{energy:.2f})'\n\n        plt.title(title,\n                  loc='left')\n        plt.xlabel('Position')\n        plt.ylabel('Probability Amplitude')\n        plt.subplots_adjust(left=0.15, right=0.95)\n        plt.grid(alpha=0.2)\n        plt.legend()\n        plt.show()\n        plt.close()\n\n    def animate(self, filename=None, ref_wf=None, ref_energy=None,\n                epoch_range=None, display_plot=True, display_progress=False):\n        # Use rich.progress.Progress to display progress.\n        column_list = [TextColumn('Animating...'),\n                       BarColumn(),\n                       TaskProgressColumn(),\n                       TimeRemainingColumn(elapsed_when_finished=True)]\n        with Progress(*column_list) as progress:\n            if epoch_range is None:\n                epoch_range = (0, len(self.losses))\n            num_frames = epoch_range[1] - epoch_range[0]\n\n            if display_progress:\n                task = progress.add_task('Animating...',\n                                         total=num_frames)\n\n            fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n\n            # Function for FuncAnimation in Matplotlib\n            def plot_frame(i):\n                if display_progress:\n                    progress.update(task, advance=1)\n\n                idx = epoch_range[0] + i\n\n                psi = self.wfs[idx]\n                norm = torch.sum(psi ** 2) * self.dx\n\n                # Plot Energy Eigenvector\n                ax = axes[0]\n                ax.clear()\n                ax.plot(to_plot(self.x),\n                        to_plot(self.wfs[idx]),\n                        'r-',\n                        label='Prediction')\n                ax.plot(to_plot(self.x),\n                        -to_plot(self.wfs[idx]),\n                        'b-',\n                        label='- Prediction')\n                if ref_wf is not None:\n                    ax.plot(to_plot(self.x),\n                            ref_wf,\n                            'k--',\n                            label='Ground Truth')\n                ax.set_title(f'Energy Eigenvector: Norm of {norm:.2f}',\n                             loc='left')\n                ax.set_xlabel('Position')\n                ax.set_ylabel('Probability Amplitude')\n                ax.set_ylim([-1.5, 1.5])\n                ax.grid(alpha=0.2)\n                ax.legend()\n\n                # Plot Energy Eigenvalue\n                ax = axes[1]\n                ax.clear()\n                ax.plot(np.arange(epoch_range[0], idx + 1),\n                        self.energies[epoch_range[0]:idx + 1])\n                if ref_energy is not None:\n                    ax.axhline(ref_energy, color='k', linestyle='--',\n                               label='Ground Truth')\n                ax.set_title(f'Energy Eigenvalue: {self.energies[idx]:.2f}',\n                             loc='left')\n                ax.set_xlabel('Epoch')\n                ax.set_ylabel('Energy Eigenvalue')\n                ax.set_xlim([epoch_range[0], epoch_range[1]])\n                ax.grid(alpha=0.2)\n                ax.legend()\n\n            ani = FuncAnimation(fig, plot_frame, frames=num_frames - 1,\n                                interval=300)\n            \n            # Save the Animation\n            if filename is None:\n                dir_str = 'sepinn_output'\n                if not os.path.isdir(dir_str):\n                    os.mkdir(dir_str)\n                time_str = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n                filename = os.path.join(dir_str, time_str + '.gif')\n            ani.save(filename, dpi=200, writer=PillowWriter(fps=50))\n            plt.close()\n            \n            if display_plot:\n                if 'google.colab' in sys.modules:\n                    filename = '/content/' + filename\n\n                display(Image(filename))\n\n\n\n\n\n\nStep 4: Define the Physical System\n# Parameters of a Quantum Harmonic Oscillator\n\nN = 500\nx0, xN = -5.0, 5.0\ndx = (xN - x0) / N\ngrid_params = x0, xN, dx, N\n\nx = torch.linspace(x0, xN, N + 1).view(-1, 1)\nk = 100\nV = 0.5 * k * x ** 2\n# Finite differences for the ground state.\n\ndiagonal = 1 / dx ** 2 + V[1:-1].detach().cpu().numpy()[:, 0]\nedge = -0.5 / dx ** 2 * np.ones(diagonal.shape[0] - 1)\nenergies, eigenvectors = eigh_tridiagonal(diagonal, edge)\n\n# Normalization of eigenvectors.\nnorms = dx * np.sum(eigenvectors ** 2, axis=0)\neigenvectors /= np.sqrt(norms)\n\neigenvectors = eigenvectors.T\n\ngnd_state = eigenvectors[0]\ngnd_energy = energies[0]\n\nx = torch.linspace(x0, xN, N - 1).view(-1, 1)\nV = 0.5 * k * x ** 2\n\n\n\nStep 5: Apply the PINN Class\nInitialize a PINN.\nparams = {'grid_params': grid_params,\n          'activation': torch.tanh,\n          'potential': V,\n          'sym': 1}\n\npinn = PINN(**params)\npinn.init_optimizer('LBFGS', lr=1e-3)\nInitiate training.\npinn.train(250)\n\n# Visualize\npinn.plot_loss()\n\n\n\n\n\nResume training.\npinn.train(250)\n\n# Visualize\npinn.plot_loss()\npinn.plot_energy()\npinn.plot_wf(ref_wf=gnd_state)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npinn.train(100)\n\n# Visualize\npinn.animate(ref_energy=gnd_energy, ref_wf=gnd_state)\n\nCompare with a PINN for which symmetry is not enforced.\nparams['sym'] = 0\n\npinn_without_sym = PINN(**params)\npinn_without_sym.init_optimizer('LBFGS', lr=1e-3)\n\npinn_without_sym.train(600)\n\npinn_without_sym.animate(ref_energy=gnd_energy, ref_wf=gnd_state)\n\n\n\n\n\nAfterword\n\nGitHub Repository\nThe repository for SE-PINN is hosted on GitHub at https://github.com/SE-PINN/SE-PINN.\n\n\nReferences\nhttps://arxiv.org/abs/2203.00451\nhttps://arxiv.org/abs/1904.08991"
  }
]